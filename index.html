<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="InfiniteWorld: A Unified Scalable Simulation Framework for General Visual-Language Robot Interaction">
  <meta name="keywords" content="imitation learning, force-control, mobile manipulation, robots ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Social Mobile Manipulation Challenge</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <h1 style="font-size: 55px; font-weight: bold; text-align: center;">CVPR 2025 Embodied AI Workshop Challenge: <br>
        Social Mobile Manipulation Challenge</h1>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors" style="padding-top: 10px;">
              <!-- <span class="author-block">
                Pengzhen Ren,
              </span>
              <span class="author-block">
                Min lin,
              </span>
              <span class="author-block">
                Zhen Luo,
              </span>
              <span class="author-block">
                Xinshuai Song,
              </span>
              <span class="author-block">
                Ziwei Chen,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Weijia Liufu,
              </span>
              <span class="author-block">
                Yixuan Yang,
              </span>
              <span class="author-block">
                Hao Zheng,
              </span>
              <span class="author-block">
                Rongtao Xu,
              </span>
              <span class="author-block">
                Zitong Huang,
              </span>
              <span class="author-block">
                Tongsheng Ding,
              </span>
              <span class="author-block">
                Luyang Xie,
              </span>
              <span class="author-block">
                Kaidong Zhang,
              </span>
              <span class="author-block">
                Changfei Fu,
              </span>
              <span class="author-block">
                Yang Liu,
              </span>
              <span class="author-block">
                Liang Lin,
              </span>
              <span class="author-block">
                Feng Zheng,
              </span>
              <span class="author-block">
                Xiaodan Liang
              </span> -->
            </div>
            <br>

            <div class="is-size-5 publication-authors">
              <span class="author-block">By &nbsp;&nbsp;</span>
              <span class="author-block">Peng Cheng Laboratory &nbsp;&nbsp;</span>
              <span class="author-block">Sun Yat-sen University &nbsp;&nbsp;</span>
              <span class="author-block">Southern University of Science and Technology &nbsp;&nbsp;</span>
              <span class="author-block">MBZUAI &nbsp;&nbsp;</span>
              <br>
              <!-- <span style="font-size: 24px;"><sup>&dagger;</sup>Equal Contribution</span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2412.05789" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span style="font-size: 22px;">arXiv</span>
                  </a>
                </span>
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="static/paper.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span style="font-size: 22px;">Paper</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/pzhren/InfiniteWorld"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span style="font-size: 22px;">Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="content has-text-justified">
      <div class="columns is-centered has-text-centered">
        <div class="container">
          <video poster="" id="benchmark1" autoplay controls muted loop playsinline width="24%"
            style="border-radius: 4%; position: relative; left: -1%;">
            <source src="static/videos/intro/calvin_0.mp4" type="video/mp4">
          </video>
          <video poster="" id="benchmark2" autoplay controls muted loop playsinline width="24%"
            style="border-radius: 4%; ; position: relative; left: -0.5%;">
            <source src="static/videos/intro/calvin_1.mp4" type="video/mp4">
          </video>
          <video poster="" id="benchmark3" autoplay controls muted loop playsinline width="24%"
            style="border-radius: 4%; position: relative; left: 0.5%;">
            <source src="static/videos/intro/calvin_2.mp4" type="video/mp4">
          </video>
          <video poster="" id="benchmark4" autoplay controls muted loop playsinline width="24%"
            style="border-radius: 4%; ; position: relative; left: 1%;">
            <source src="static/videos/intro/calvin_3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item open_the_drawer">
              <video poster="" id="open_the_drawer" autoplay controls muted loop playsinline height="90%">
                <source src="static/videos/real_exp/open_the_drawer.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item 3task_distractor">
              <video poster="" id="3task_distractor" autoplay controls muted loop playsinline height="90%">
                <source src="static/videos/intro/3task_distractor.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item 3task_unseen">
              <video poster="" id="3task_unseen" autoplay controls muted loop playsinline height="90%">
                <source src="static/videos/intro/3task_unseen.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item 3task_normal">
              <video poster="" id="3task_normal" autoplay controls muted loop playsinline height="90%">
                <source src="static/videos/intro/3task_normal.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>


      <div class="columns is-centered has-text-centered">
        <img id="robot_result" width="100%" src="static/images/challenge.png" />
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p style="font-size: 24px;">
              Figure 1: The dynamic interaction environment that agents will navigate in both human-robot and
              robot-robot tasks.
            </p>
          </div>
        </div>
      </div>


      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-1">Social Mobile Manipulation Challenge</h2>
          <div class="content has-text-justified">
            <p style="font-size: 24px;">
              The 1st Social Mobile Manipulation Challenge focuses on developing embodied AI agents capable
              of performing long sequences of complex tasks through social interactions. These tasks involve reasoning
              about human intentions and planning within dynamic, multi-agent environments (Figure 1).
              The challenge includes two interaction modes: human-robot and robot-robot. The goal is to advance
              the community's capabilities in areas like human intention reasoning, long-term task planning, and
              effective social interaction between embodied agents.
            </p>
          </div>
        </div>
      </div>
  </section>

  <section>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Dataset</h2>
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            We will provide two main datasets for this challenge. The first dataset includes various human-robot
            interaction tasks, designed for hierarchical knowledge-based interactions.
            The second dataset involves robot-robot interactions where multiple agents collaborate on equal
            terms to complete tasks.
            <br>These datasets are currently under development and are expected to be released by 03/01/2025.
            In case of delays, we plan to provide smaller, simplified versions of the datasets to ensure
            participants can begin their work on time.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Ethical considerations</h2>
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            Since the challenge involves the development of autonomous agents,
            careful attention is required when transferring policies trained in simulation to real-world environments.
            Proper validation processes should be established to avoid unintended behaviors
            in robots, especially in human-robot interaction scenarios.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Submission evaluation</h2>
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            Participants will submit their solution files, which will be evaluated
            using our simulation platform based on NVIDIA's Isaac Sim. We will use the evalAI platform
            to manage submissions and rerun code for the top submissions. Participants are expected to
            submit both their code and a ReadMe file with clear instructions on how to execute their solution.
          </p>
        </div>
      </div>
    </div>
  </section>


  <section>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Timeline</h2>
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            The challenge will commence on 01/02/2025, with submissions due by 01/05/2025.
            Final decisions will be shared with participants by 01/06/2025.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Challenge organizers</h2>
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            The challenge is organized by Xiaodan Liang (MBZUAI), Rongtao Xu
            (MBZUAI), Pengzhen Ren (Pengcheng Laboratory), Bingqian Lin (Shanghai Jiao Tong University), Xiaojun Chang
            (University of Technology Sydney), Ivan Laptev (MBZUAI), Ian Reid(MBZUAI), Cewu Lu (Shanghai Jiao Tong
            University), Mingfei Han (MBZUAI), and Xiwen
            Liang (Sun Yat-sen University).
          </p>
        </div>
      </div>
    </div>
  </section>





  <!-- <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Method</h2>
        <video poster="" id="overview" autoplay muted loop playsinline width="70%">
          <source src="static/videos/intro/overview.mp4" type="video/mp4">
        </video>
        <div class="content is-centered has-text-justified">
          <p style="font-size: 24px;">
            <br>
            GR-1 is first pre-trained on the task of video prediction with a large-scale video dataset.
            It is then finetuned on robot data to learn multi-task visual robot manipulation.
            GR-1 is a simple GPT-style transformer which is able to take different modalities as inputs and outputs
            future images and actions.
            <br>
          </p>
        </div>
        <div class="columns is-centered has-text-centered">
          <img id="encoder_decoder" width="80%" src="static/images/encoder_decoder.png" />
        </div>
        <div class="content is-centered has-text-justified">
          <p style="font-size: 24px;">
            The language input is encoded via CLIP text encoder.
          </p>
        </div>
      </div>
    </div>
  </section> -->

  <!-- <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Real Robot Experiments</h2>
        <img id="robot_result" width="80%" src="static/images/real_robot.png">
        <div class="content has-text-justified">
          <br>
          <p style="font-size: 24px;">
            We perform extensive end-to-end real robot experiments to evaluate how GR-1 works in the real world.
          </p>
          <div class="columns is-centered has-text-centered">
            <img id="robot_result" width="40%" src="static/images/real_results.png" />
          </div>
          <br>
          <p style="font-size: 24px;">
            <b>1. Seen Objects</b><br>
            In this setting, there are three objects, i.e. a broccoli, an eggplant, and a bell pepper.
          </p>
          <br>
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item unseen_cat_0">

                <video poster="" id="unseen_cat_0" autoplay controls muted loop playsinline height="90%">
                  <source src="static/videos/real_exp/normal_put_the_bell_pepper_onto_the_plate.mp4" type="video/mp4">
                </video>
              </div>
              <div class="item unseen_cat_1">

                <video poster="" id="unseen_cat_1" autoplay controls muted loop playsinline height="90%">
                  <source src="static/videos/real_exp/distractor_put_the_broccoli_onto_the_desk.mp4" type="video/mp4">
                </video>
              </div>
              <div class="item unseen_inst_0">

                <video poster="" id="unseen_inst_0" autoplay controls muted loop playsinline height="90%">
                  <source src="static/videos/real_exp/bg_put_the_bell_pepper_onto_the_plate.mp4" type="video/mp4">
                </video>
              </div>
              <div class="item unseen_inst_1">

                <video poster="" id="unseen_inst_1" autoplay controls muted loop playsinline height="90%">
                  <source src="static/videos/real_exp/normal_put_the_eggplant_onto_the_plate.mp4" type="video/mp4">
                </video>
              </div>
              <div class="item unseen_inst_1">

                <video poster="" id="unseen_inst_1" autoplay controls muted loop playsinline height="90%">
                  <source src="static/videos/real_exp/distractor_put_the_eggplant_onto_the_plate.mp4" type="video/mp4">
                </video>
              </div>
              <div class="item unseen_inst_1">

                <video poster="" id="unseen_inst_1" autoplay controls muted loop playsinline height="90%">
                  <source src="static/videos/real_exp/bg_put_the_eggplant_onto_the_desk.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
          <p style="font-size: 24px;">
            <br>
            <b>2. Unseen Instances & Unseen Categories</b><br>
            In these two settings, we challenge the robot to transport unseen objects.
            <br>
          </p>
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item unseen_cat_0">

                <video poster="" id="unseen_cat_0" autoplay controls muted loop playsinline height="90%">
                  <source src="static/videos/real_exp/unseen_cat_put_the_tomato_onto_the_plate.mp4" type="video/mp4">
                </video>
              </div>
              <div class="item unseen_cat_1">

                <video poster="" id="unseen_cat_1" autoplay controls muted loop playsinline height="90%">
                  <source src="static/videos/real_exp/unseen_cat_put_the_yellow_peach_onto_the_desk.mp4"
                    type="video/mp4">
                </video>
              </div>
              <div class="item unseen_inst_0">

                <video poster="" id="unseen_inst_0" autoplay controls muted loop playsinline height="90%">
                  <source src="static/videos/real_exp/unseen_inst_put_the_bell_pepper_onto_the_plate.mp4"
                    type="video/mp4">
                </video>
              </div>
              <div class="item unseen_inst_1">

                <video poster="" id="unseen_inst_1" autoplay controls muted loop playsinline height="90%">
                  <source src="static/videos/real_exp/unseen_inst_put_the_eggplant_onto_the_plate.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>

          <p style="font-size: 24px;">
            <br>
            <b>3. Articulated Object Manipulation</b><br>
            In this experiment, we aim to evaluate GR-1 on handling contact-rich articulated object manipulation.
          </p>

          <div class="container">
            <video poster="" id="open_the_drawer" autoplay controls muted loop playsinline width="33%"
              style="border-radius: 4%; position: relative; left: 16%;">
              <source src="static/videos/real_exp/open_the_drawer.mp4" type="video/mp4">
            </video>
            <video poster="" id="close_the_drawer" autoplay controls muted loop playsinline width="33%"
              style="border-radius: 4%; position: relative; left: 17%;">
              <source src="static/videos/real_exp/close_the_drawer.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->


  <!-- 
  <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">CALVIN Benchmark Experiments</h2>
        <img id="calvin_env" width="70%" src="static/images/calvin_env.png"
          style="display: block; margin-left: auto; margin-right: auto">
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            <br>
            <a href="http://calvin.cs.uni-freiburg.de/"><b>CALVIN</b></a> is a challenging benchmark focusing on
            learning language-conditioned policy for long-horizon robot manipulation.
          </p>
          <div class="columns">
            <div class="column has-text-left">
              <img src="static/images/ABCD_D.png" class="interpolation-image" alt=""
                style="display: block; margin-left: auto; margin-right: auto" width="70%" />
            </div>
            <div class="column has-text-left">
              <img src="static/images/ABC_D.png" class="interpolation-image" alt=""
                style="display: block; margin-left: auto; margin-right: auto" width="70%" />
            </div>
          </div>
          <div class="content has-text-justified">
            <b style="font-size: 24px;">1. Long-Horizon Multi-Task Learning (ABCD->D)</b> <br>
            <p style="font-size: 24px;">
              In this setting, the robot is trained with data collected from environments A, B, C, and D and evaluated
              in environment D.<br>
            </p>
          </div>
          <div class="content has-text-justified">
            <b style="font-size: 24px;">2. Zero-Shot Generalization to Unseen Scenes (ABC->D)</b> <br>
            <p style="font-size: 24px;">
              We hypothesize that the strong generalization capability stems from being exposed to large-scale data in
              pre-training.<br>
            </p>
          </div>
          <div class="columns">
            <div class="column has-text-left">
              <img src="static/images/10_percent_data.png" class="interpolation-image" alt=""
                style="display: block; margin-left: auto; margin-right: auto" width="70%" />
            </div>
            <div class="column has-text-left">
              <img src="static/images/unseen_lang.png" class="interpolation-image" alt=""
                style="display: block; margin-left: auto; margin-right: auto" width="70%" />
            </div>
          </div>
          <div class="content has-text-justified">
            <b style="font-size: 24px;">3. Data Efficiency (10% ABCD->D)</b><br>
            <p style="font-size: 24px;">
              Robot data is sparse.
            </p>
          </div>
          <div class="content has-text-justified">
            <b style="font-size: 24px;">4. Zero-Shot Generalization to Unseen Languages</b><br>
            <p style="font-size: 24px;">
              Human languages are diverse, i.e. there are different ways of describing a task.
              We aim to test whether GR-1 is able to generalize to languages that are different from those seen in
              training.
              We leveraged <a href="https://openai.com/research/gpt-4"><b>GPT-4</b></a> to generate synonymous
            </p>
          </div>
          <div class="columns is-centered has-text-centered" style="padding-top: 30px;">
            <img id="framework" width="60%" src="static/images/unseen_lang_exp.png">
          </div>
        </div>
      </div>
    </div>
  </section> -->


  <!-- 
  <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Ablation Studies and Video Prediction Results</h2>
        <div class="content has-text-justified">
          <div class="column has-text-left" style="padding-top: 25px;">
            <p style="font-size: 24px;">
              GR-1 features video prediction and large-scale pretraining on video prediction.
              <br><br>
            </p>
          </div>
          <div class="columns">
            <div class="column has-text-left">
              <img src="static/images/ablation_ABCD_D.png" class="interpolation-image" alt=""
                style="display: block; margin-left: auto; margin-right: auto" />
            </div>
            <div class="column has-text-left">
              <img src="static/images/ablation_ABC_D.png" class="interpolation-image" alt=""
                style="display: block; margin-left: auto; margin-right: auto" />
            </div>
            <div class="column has-text-left">
              <img src="static/images/ablation_10_percent_data.png" class="interpolation-image" alt=""
                style="display: block; margin-left: auto; margin-right: auto" />
            </div>
          </div>
          <div class="column has-text-left" style="padding-top: 25px;">
            <p style="font-size: 24px;">
              We probe into GR-1 to investigate its video prediction performance on CALVIN and real robot data.
            </p>
          </div>
          <img id="robot_result" width="65%" style="display: block;margin-left: auto; margin-right: auto;"
            src="static/images/fwd_pred.png">
        </div>
      </div>
    </div>
  </section> -->

  <!-- <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Conclusions and Future Work</h2>
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            In this paper, we propose to leverage large-scale video generative pre-training for enhancing visual robot
            manipulation learning
          </p>
        </div>
      </div>
    </div>
  </section> -->

  <!-- 
  <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title" style="text-align: left;">BibTeX</h2>
        <div class="content has-text-justified">
          <pre><code style="font-size: 24px;">@misc{wu2023unleashing,
      title={Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation}, 
      author={Hongtao Wu and Ya Jing and Chilam Cheang and Guangzeng Chen and Jiafeng Xu and Xinghang Li and Minghuan Liu and Hang Li and Tao Kong},
      year={2023},
      eprint={2312.13139},
      archivePrefix={arXiv},
      primaryClass={cs.RO}}
          </code></pre>
        </div>
      </div>
    </div>
  </section> -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="content">
          <p style="font-size: 24px;">
            The website template was adapted from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>